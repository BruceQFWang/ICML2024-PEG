MODEL:  
  ANCESTRY: 'deit_small_patch16_224'
  TYPE: deit
  NAME: deit_learngene_small
  DEIT:
    EMBED_DIM: 384
    DEPTHS: 6 # 6 layers
    NUM_HEADS: 6
    NUM_HEADS_LEARNGENE: 4
    NUM_HEADS_DESCENDANT: 6
  HDP:
    # HDP (str): whether to use HDP / which type to use, default='' meaning not using
    HDP: 'qkv' # ''
    # HDP_RATIOS (list of int/float): how much to scale down hdp head count | only edit the first number
    HDP_RATIOS: [1.5, 1]  # To set up two linear layers, Linear(num_heads_descendant/HDP_RATIOS[0], num_heads_descendant/HDP_RATIOS[1]) and Linear(num_heads_descendant/HDP_RATIOS[1], num_heads_descendant)
    FFN_RATIOS: 2.0 # 1.3333
    DESCENDANT_FFN_RATIOS: 1.5 # 1.34
    FFN_INHERIT: 'expand' # 'direct'
    # NON_LINEAR (bool): whether to use ReLU and multi HDP layers 
    NON_LINEAR: True
    DISTRIBUTION: 'Gaussian-Layer' # Uniform
    
DATA:
  # BATCH_SIZE (int): default=128
  BATCH_SIZE: 200
  DATASET: 'CIFAR100'
  DATA_PATH: '/home/xxxx/datasets'
TRAIN:
  # lr is for ebs=512, will scale linearly
  # default: warmup_lr (5e-7) -> base_lr (5e-4) -> min_lr (5e-6)
  BASE_LR: 5e-4
  WARMUP_LR: 5e-7
  MIN_LR: 5e-6
  # WARMUP_EPOCHS (int): default=20
  WARMUP_EPOCHS: 10

TAG : 'CIFAR100_2gpu_lr=5e-4_warm=10_epochs=500'