MODEL:  
  ANCESTRY: 'deit_small_patch16_224' # 'deit_small_attn4_ffn12'
  TYPE: deit
  NAME: deit_learngene_small
  DEIT:
    EMBED_DIM: 384
    DEPTHS: 12
    NUM_HEADS: 6
    NUM_HEADS_LEARNGENE: 4
    NUM_HEADS_DESCENDANT: 6
  HDP:
    # HDP (str): whether to use HDP / which type to use, default='' meaning not using
    HDP:  'qkv' # ''
    # HDP_RATIOS (list of int/float): how much to scale down hdp head count | only edit the first number
    HDP_RATIOS: [1.5, 1]  # To set up two linear layers, Linear(num_heads_descendant/HDP_RATIOS[0], num_heads_descendant/HDP_RATIOS[1]) and Linear(num_heads_descendant/HDP_RATIOS[1], num_heads_descendant)
    FFN_RATIOS: 2.0 # 1.3333
    DESCENDANT_FFN_RATIOS: 2.0 # 1.34
    FFN_INHERIT: 'expand' # 'direct'
    # NON_LINEAR (bool): whether to use ReLU and multi HDP layers 
    NON_LINEAR: True
    DISTRIBUTION: 'Gaussian-Layer' # Uniform
    
DATA:
  # BATCH_SIZE (int): default=128
  BATCH_SIZE: 200
TRAIN:
  # lr is for ebs=512, will scale linearly
  # default: warmup_lr (5e-7) -> base_lr (5e-4) -> min_lr (5e-6)
  BASE_LR: 5e-4
  WARMUP_LR: 5e-7
  MIN_LR: 5e-6
  # WARMUP_EPOCHS (int): default=20
  WARMUP_EPOCHS: 5

TAG : '4gpu_lr=5e-4_warm=5_attn4to6_ffn6to12_initialize_100epochs'