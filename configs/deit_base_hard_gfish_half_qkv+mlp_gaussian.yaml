MODEL:  
  ANCESTRY: 'deit_base_patch16_224'
  TYPE: deit
  NAME: deit_learngene_base
  # DROP_PATH_RATE: 0.3
  # SWIN:
  #   EMBED_DIM: 96
  #   DEPTHS: [ 2, 2, 18, 2 ]
  #   NUM_HEADS: [ 3, 6, 12, 24 ]
  #   NUM_HEADS_LEARNGENE: [ 3, 6, 6, 12 ]
  #   NUM_HEADS_DESCENDANT: [ 3, 6, 12, 24 ]
  #   WINDOW_SIZE: 7
  DEIT:
    EMBED_DIM: 768
    DEPTHS: 12
    NUM_HEADS: 12
    NUM_HEADS_LEARNGENE: 8
    NUM_HEADS_DESCENDANT: 12
  HDP:
    # HDP (str): whether to use HDP / which type to use, default='' meaning not using
    HDP: 'qkv' #''
    # HDP_RATIOS (list of int/float): how much to scale down hdp head count | only edit the first number
    HDP_RATIOS: [1.5, 1]  # 设置两层linear，分别是Linear(num_heads_descendant/HDP_RATIOS[0], num_heads_descendant/HDP_RATIOS[1])和Linear(num_heads_descendant/HDP_RATIOS[1], num_heads_descendant)
    FFN_RATIOS: 2.0 # 1.3333
    DESCENDANT_FFN_RATIOS: 2.0 # 1.34
    FFN_INHERIT: 'expand' # 'direct'
    # STAGES (list of int): the stages to apply hdp to
    # STAGES: [2, 3]
    # NON_LINEAR (bool): whether to use ReLU and multi HDP layers 
    NON_LINEAR: True
    DISTRIBUTION: 'Gaussian-Layer' # Uniform
    # MLP_EXPAND: [2, 3] # only expand the MLP in stage 3
  #PRETRAINED: './checkpoint/swin/swin_tiny_patch4_window7_224.pth'
    
DATA:
  # BATCH_SIZE (int): default=128
  BATCH_SIZE: 100
TRAIN:
  # lr is for ebs=512, will scale linearly
  # default: warmup_lr (5e-7) -> base_lr (5e-4) -> min_lr (5e-6)
  BASE_LR: 5e-4 # 5e-4
  WARMUP_LR: 5e-7
  MIN_LR: 5e-6  # 5e-6
  # WARMUP_EPOCHS (int): default=20
  WARMUP_EPOCHS: 5

TAG : '4gpu_lr=5e-4_warm=5_attn8to12_ffn6to12_initialize_100epochs'