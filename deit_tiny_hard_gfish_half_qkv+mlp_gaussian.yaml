MODEL:  
  ANCESTRY:  'deit_tiny_patch16_224' # 'deit_tiny_attn2_ffn9'
  TYPE: deit
  NAME: deit_learngene_tiny
  DEIT:
    EMBED_DIM: 192
    DEPTHS: 6
    NUM_HEADS: 3
    NUM_HEADS_LEARNGENE: 2
    NUM_HEADS_DESCENDANT: 3
  HDP:
    # HDP (str): whether to use HDP / which type to use, default='' meaning not using
    HDP: 'qkv' # '' 
    # HDP_RATIOS (list of int/float): how much to scale down hdp head count | only edit the first number
    HDP_RATIOS: [1.5, 1]  # To set up two linear layers, Linear(num_heads_descendant/HDP_RATIOS[0], num_heads_descendant/HDP_RATIOS[1]) and Linear(num_heads_descendant/HDP_RATIOS[1], num_heads_descendant)
    # NON_LINEAR (bool): whether to use ReLU and multi HDP layers 
    FFN_RATIOS: 2.0 # 1.3333
    DESCENDANT_FFN_RATIOS: 2.0 # 1.34
    FFN_INHERIT: 'expand' # 'direct'
    NON_LINEAR: True
    DISTRIBUTION: 'Gaussian-Layer' # Uniform
    # MLP_EXPAND: [2, 3] # only expand the MLP in stage 3
  #PRETRAINED: './checkpoint/swin/swin_tiny_patch4_window7_224.pth'
    
DATA:
  # BATCH_SIZE (int): default=128
  BATCH_SIZE: 1000
TRAIN:
  # lr is for ebs=512, will scale linearly
  # default: warmup_lr (5e-7) -> base_lr (5e-4) -> min_lr (5e-6)
  BASE_LR: 5e-4
  WARMUP_LR: 5e-7
  MIN_LR: 5e-6
  # WARMUP_EPOCHS (int): default=20d
  WARMUP_EPOCHS: 5

TAG : '4gpu_lr=5e-4_warm=5_attn2to3_ffn6to12_initialize_100epochs' 